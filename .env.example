# AI Memory Architecture - Environment Variables
# Copy this file to .env and configure your preferred LLM provider

# ============================================
# LLM Provider Options (choose one or more)
# ============================================

# Option 1: OpenAI (Cloud - Paid)
# Get from: https://platform.openai.com/api-keys
# OPENAI_API_KEY=sk-proj-your-key-here

# Option 2: Google Gemini (Cloud - Free tier available)
# Get from: https://aistudio.google.com/app/apikey
# GEMINI_API_KEY=your-gemini-key-here

# Option 3: Ollama (Local - Free)
# Install from: https://ollama.ai
# Run: ollama pull llama2
# OLLAMA_URL=http://localhost:11434
# OLLAMA_MODEL=llama2

# Option 4: Local Fallback (No LLM - Rule-based responses)
# Set to "true" to enable simple rule-based responses when no LLM is available
USE_LOCAL_FALLBACK=true

# ============================================
# Notes:
# ============================================
# - If no API keys are set, the system will use local fallback mode
# - Local fallback provides basic conversational responses
# - For best results, use OpenAI, Gemini, or Ollama
# - Ollama is recommended for fully local operation
