# AI Memory Architecture

A production-ready long-term memory system for AI assistants that extracts, stores, and retrieves structured user information from conversations using semantic search and LLM-powered extraction.

## ğŸ¯ Overview

This system enables AI assistants to maintain persistent memory across conversations by:
- **Extracting** structured memories from natural language conversations
- **Storing** memories with semantic embeddings for efficient retrieval
- **Retrieving** relevant context based on user queries
- **Orchestrating** memory-augmented chat responses via FastAPI

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    System Orchestrator                       â”‚
â”‚                      (FastAPI Layer)                         â”‚
â”‚  â€¢ Chat endpoint with memory context                         â”‚
â”‚  â€¢ Prompt injection & LLM inference                          â”‚
â”‚  â€¢ Latency monitoring & metrics                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Memory Manager                            â”‚
â”‚  â€¢ MemoryEngine: Storage & retrieval                         â”‚
â”‚  â€¢ VectorStore: FAISS-based semantic search                  â”‚
â”‚  â€¢ Embedding Service: sentence-transformers                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Memory Extractor                          â”‚
â”‚  â€¢ LLM-based extraction (OpenAI/Gemini)                      â”‚
â”‚  â€¢ Structured JSON output with validation                    â”‚
â”‚  â€¢ Fallback to rule-based extraction                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Components

### 1. Memory Extractor (`extractor/`)
- Processes conversations using LLMs (OpenAI GPT-4o-mini or Google Gemini)
- Extracts structured memories following strict JSON schemas
- Categorizes into: preferences, facts, constraints, commitments

### 2. Memory Manager (`memory_manager/`)
- **MemoryEngine**: Core storage and retrieval logic
- **VectorStore**: FAISS-based vector database for semantic search
- **Embedding Service**: Local embeddings using sentence-transformers
- Handles add/update/merge operations with deduplication

### 3. System Orchestrator (`orchestrator/`) â­ NEW
- **FastAPI Backend**: Production-ready REST API
- **ChatOrchestrator**: Connects all components
- **LLMClient**: Unified client for OpenAI and Gemini
- **PromptBuilder**: Context-aware prompt construction
- **Logging & Metrics**: Performance monitoring

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Create virtual environment
python -m venv venv

# Activate (Windows)
venv\Scripts\activate

# Activate (Linux/Mac)
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure API Keys

Create a `.env` file:
```env
OPENAI_API_KEY=your_openai_key_here
GEMINI_API_KEY=your_gemini_key_here
```

### 3. Run the Orchestrator

```bash
# Quick start
python run_orchestrator.py

# Or manually
python -m orchestrator.main
```

Server starts at: `http://localhost:8000`

API Docs: `http://localhost:8000/docs`

## ğŸ“¡ API Endpoints

### Chat with Memory Context
```bash
POST /chat/
{
  "user_id": "user123",
  "message": "Hi, I'm Sarah from Tokyo. I prefer email notifications."
}
```

Response:
```json
{
  "response": "Hello Sarah! Nice to meet you...",
  "memories_used": 3,
  "latency_ms": 1250,
  "metadata": {
    "retrieval_ms": 45,
    "llm_ms": 1100,
    "extraction_ms": 105
  }
}
```

### Retrieve Memories
```bash
POST /chat/retrieve
{
  "user_id": "user123",
  "query": "user preferences",
  "top_k": 5
}
```

### Health Check
```bash
GET /health
```

### Performance Metrics
```bash
GET /metrics
```

## ğŸ§ª Testing

### Test the Orchestrator API
```bash
# Start server
python run_orchestrator.py

# In another terminal, run tests
python tests/test_orchestrator.py
```

### Test Memory Engine
```bash
python tests/test_memory_engine.py
```

### Test Memory Flow
```bash
python tests/demo_memory_flow.py
```

## ğŸ“Š Memory Types

The system categorizes memories into 4 types:

### Preferences
- preferred_language, communication_style, timezone
- call_time_preference, contact_method, notification_preference

### Facts
- user_name, location, occupation
- education, company, device_used

### Constraints
- no_calls_time_range, do_not_contact_days
- dietary_restriction, access_limitation, budget_limit

### Commitments
- reminder_request, scheduled_call
- task_deadline, follow_up_request

## ğŸ”§ Configuration

### Memory Retrieval
- `top_k = 5`: Number of memories to retrieve
- `score_threshold = 0.3`: Minimum relevance score
- `MAX_MEMORIES = 5`: Maximum memories in context

### LLM Settings
- `temperature = 0.7`: Sampling temperature
- `model = "gpt-4o-mini"`: OpenAI model
- `model = "gemini-2.0-flash"`: Gemini model

## ğŸ“ˆ Performance

Typical latencies:
- Memory retrieval: ~50ms
- LLM inference: ~1000-1500ms
- Memory extraction: ~100-200ms
- **Total end-to-end: ~1.2-2.0s**

## ğŸ› ï¸ Tech Stack

- **FastAPI**: REST API framework
- **OpenAI / Gemini**: LLM providers
- **FAISS**: Vector similarity search
- **sentence-transformers**: Local embeddings
- **Pydantic**: Data validation
- **uvicorn**: ASGI server

## ğŸ“ Project Structure

```
AI-Memory-Architecture/
â”œâ”€â”€ orchestrator/           # FastAPI orchestration layer
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ routes/        # API endpoints
â”‚   â”‚   â””â”€â”€ models/        # Request/response models
â”‚   â”œâ”€â”€ services/          # Core services
â”‚   â”‚   â”œâ”€â”€ orchestrator.py
â”‚   â”‚   â”œâ”€â”€ llm_client.py
â”‚   â”‚   â””â”€â”€ prompt_builder.py
â”‚   â””â”€â”€ middleware/        # Logging middleware
â”œâ”€â”€ memory_manager/        # Memory storage & retrieval
â”‚   â”œâ”€â”€ memory_engine.py
â”‚   â”œâ”€â”€ vector_store.py
â”‚   â””â”€â”€ embedding_service.py
â”œâ”€â”€ extractor/             # Memory extraction
â”‚   â””â”€â”€ extract_memory.py
â”œâ”€â”€ prompts/               # LLM prompts
â”œâ”€â”€ schema/                # JSON schemas
â”œâ”€â”€ tests/                 # Test scripts
â””â”€â”€ run_orchestrator.py    # Quick start script
```

## ğŸ” Security Notes

- Store API keys in `.env` file (never commit)
- Configure CORS appropriately for production
- Implement authentication for production use
- Rate limit API endpoints

## ğŸ“ License

MIT License

## ğŸ¤ Contributing

Contributions welcome! Please open an issue or PR.

## ğŸ“§ Support

For issues or questions, please open a GitHub issue.

---

**Built with â¤ï¸ for memory-augmented AI systems**
